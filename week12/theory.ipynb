{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: Dimensionality Reduction with Autoencoder\n",
    "\n",
    "Write an autoencoder in pytorch and use it to compute a dimensionality reduction for the MNIST digits (as used in PCA exercise) to $k$ numbers (hidden layer size). \n",
    "Then compare to PCA by plotting the reconstructed data points\n",
    "\n",
    "Technically you must complete the autoencoder class below.\n",
    "* Implement cost, encode and decode in the AutoEncoder class.      \n",
    "    For n data points with dimension d the cost is \n",
    "    $$\n",
    "    \\frac{1}{dn} \\sum_{i=1}^n \\sum_{j=1}^d (x_{i, j} - \\hat{x}_{i, j})^2 \n",
    "    $$\n",
    "    where $\\hat{x} = \\textrm{dec}(\\textrm{enc}(x))$ the result of encoding and then decoding $x$. Note that we normalize by $1/dn$ and not just $1/n$ like in the lecture. This is purely a practical choice that makes it easier to deal with learning rates.\n",
    "    Let $k$ denote the hidden layer size then:\n",
    "    - The encoder is given as  $\\textrm{enc}(x) = \\textrm{relu}(x W_1 + b_1)$, where $W_1$ is a matrix of size (784, k) and $b_1$ is of size (1, k)\n",
    "    - Similarly the decoder is $\\textrm{dec}(x) = (x W_2 + b_2)$ where $W_2$ is a matrix of size (k, 784) and $b_2$ is of size (1, 784)\n",
    "* Test the code by running simple_test and set hidden size to 16 and epochs to at least 10. It may take some time to actually fit. \n",
    "\n",
    "In the first cell we load the MNIST OCR data in pytorch way and show how to use the data iterators.\n",
    "The autoencoder class is in the next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def plot_images(dat, k=16):\n",
    "    \"\"\" Plot the first k vectors as 28 x 28 images \"\"\"\n",
    "    size = 28 \n",
    "    x2 = dat[0:k,:].reshape(-1, size, size)\n",
    "    x2 = x2.transpose(1, 0, 2)\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    ax.imshow(x2.reshape(size, -1), cmap='bone')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=False)\n",
    "# works like this \n",
    "for idx, (X, y) in enumerate(train_loader):\n",
    "    x_vec = X.reshape(-1, 784)\n",
    "    print('Input Images')\n",
    "    plot_images(x_vec.numpy(), k=x_vec.shape[0]) # move to numpy only relevant when needing data in that format\n",
    "    if idx > 5:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class AutoEncoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" The parameters required to be set by fit method \"\"\"\n",
    "        self.W1 = None\n",
    "        self.b1 = None\n",
    "        self.W2 = None\n",
    "        self.b2 = None\n",
    "\n",
    "    def cost(self, X, W1, b1, W2, b2):\n",
    "        \"\"\" Compute (coordinate-wise) Least Squares Loss of reconstructing the input.\n",
    "        The clamp function may be useful\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          W1: torch.tensor shape (d, h) - weights\n",
    "          b1: torch.tensor shape (1, h) - bias weight\n",
    "          W2: torch.tensor shape (h, d) - weights\n",
    "          b2: torch.tensor shape (1, d) - bias weight\n",
    "        returns pytorch tensor with least squared cost\n",
    "        \"\"\"\n",
    "   \n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, data_loader, hidden_size=32, epochs=5):   \n",
    "        \"\"\" GD Learning Algorithm with pytorch\n",
    "        \n",
    "         Args:\n",
    "         data_loader: torch dataloader allows enumeration over data\n",
    "         hidden_size: int\n",
    "         epochs: int \n",
    "         \n",
    "         sets \n",
    "        \"\"\"\n",
    "        def my_init(s_to, s_from):\n",
    "            \"\"\" Standard way to initialize matrices in neural nets - you can ignore it \"\"\"\n",
    "            w = torch.zeros(s_to, s_from)\n",
    "            b = torch.zeros(s_to, 1)\n",
    "            nn.init.kaiming_uniform_(w, a=np.sqrt(5))\n",
    "            bound = 1 / np.sqrt(s_from)\n",
    "            nn.init.uniform_(b, -bound, bound)\n",
    "            return torch.transpose(w, 1, 0), torch.transpose(b, 1, 0)        \n",
    "        \n",
    "        W1, b1 = my_init(hidden_size, 784)\n",
    "        W2, b2 = my_init(784, hidden_size)\n",
    "        for i, z in enumerate([W1, b1, W2, b2]):\n",
    "            z.requires_grad_()\n",
    "\n",
    "        sgd = optim.SGD(params={W1, b1, W2, b2}, lr=0.1, weight_decay=1e-4)\n",
    "        #sgd = optim.AdamW(params={W1, b1, W2, b2}, lr=0.0001, weight_decay=1e-4)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_count = 0\n",
    "            running_loss = 0\n",
    "            for idx, (X, y) in enumerate(data_loader):\n",
    "                sgd.zero_grad()\n",
    "                inputs = X.view(-1, 784) \n",
    "                loss = self.cost(inputs, W1, b1, W2, b2)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_count += 1\n",
    "                running_loss += loss.item()\n",
    "                if idx % 10 == 9:   \n",
    "                    print('Running loss: {2:.3f}'.format(epoch + 1, idx + 1,  epoch_loss/epoch_count), end='\\r')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                loss.backward()\n",
    "                sgd.step()\n",
    "            print('Epoch: {0}, Mean Least Square loss: {1}'.format(epoch + 1, epoch_loss/epoch_count))\n",
    "\n",
    "        self.W1 = W1.detach() #.numpy()\n",
    "        self.W2 = W2.detach() #.numpy()\n",
    "        self.b1 = b1.detach() #.numpy()\n",
    "        self.b2 = b2.detach() #.numpy()\n",
    "        \n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\" Compute the embedded inputs.\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         \n",
    "        Returns:\n",
    "         decoded: torch.tensor shape (n, h) using self.W1, self.b1 and ReLU\n",
    "        \"\"\"\n",
    "\n",
    "        encoded = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, X):\n",
    "        \"\"\" Compute the reconstructed inputs from the encoding.\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, h)\n",
    "         \n",
    "        Returns:\n",
    "         decoded: torch.tensor shape (n, d) using self.W2, self.b2\n",
    "        \"\"\"\n",
    "\n",
    "        decoded = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return decoded\n",
    "    \n",
    "def simple_test(hidden_size=32, epochs=1):\n",
    "    net = AutoEncoder()\n",
    "    net.fit(data_loader=train_loader, hidden_size=hidden_size,  epochs=epochs)\n",
    "    X_sample, y_sample =  next(iter(train_loader))\n",
    "    print(X_sample.shape)\n",
    "    x_vec = X_sample.view(-1, 784)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_sample = net.decode(net.encode(x_vec)).numpy()\n",
    "    print('Input Images')\n",
    "    plot_images(x_vec.numpy(), k=x_vec.shape[0])    \n",
    "    print('Reconstructed Images')\n",
    "    plot_images(reconstructed_sample, k=x_vec.shape[0])\n",
    "    \n",
    "    #data, labels =\n",
    "    data = dataset1.data.view(-1,784).type(torch.FloatTensor)\n",
    "    labels = dataset1.targets\n",
    "    # reduce size for speed\n",
    "    rp = np.random.permutation(len(labels))\n",
    "    train_dat = data[rp[:5000],:]\n",
    "    test_dat = data[rp[5000:6000], :]\n",
    "    train_lab = labels[rp[:5000]].numpy()\n",
    "    test_lab = labels[rp[5000:6000]].numpy()\n",
    "    \n",
    "    enc_train_dat = net.encode(train_dat)\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "    clf.fit(enc_train_dat, train_lab)\n",
    "    enc_test_dat = net.encode(test_dat)\n",
    "    acc = (clf.predict(enc_test_dat) == test_lab).mean()\n",
    "    print('AutoEncoder of dimension',hidden_size)\n",
    "    print('Test Accuracy using AutoEncoder:', 100*acc)\n",
    "    \n",
    "\n",
    "simple_test(32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: Random Projections Experiment\n",
    "In the following, you must implement the random projection known as the Johnson-Lindenstrauss transform.\n",
    "\n",
    "The code you must implement:\n",
    "1. Fill out the random projection matrix Z such that a data matrix X can be embedded as XZ. Z should have the right scaling and normal distributed entries. Z should have k columns (the target dimension for embedding, called k in the lectures).\n",
    "2. Project the training data using Z\n",
    "3. Train the SGD classifier on the projected training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Load full dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "#data, labels =\n",
    "data = mnist_trainset.data.numpy().reshape((60000, 28*28))\n",
    "labels = mnist_trainset.targets.numpy()\n",
    "\n",
    "# reduce size for speed\n",
    "rp = np.random.permutation(len(labels))\n",
    "dat = data[rp[:6000], :]\n",
    "lab = labels[rp[:6000]]\n",
    "\n",
    "train_dat = dat[:5000,:]\n",
    "test_dat = dat[5000:, :]\n",
    "train_lab = lab[:5000]\n",
    "test_lab = lab[5000:]\n",
    "\n",
    "for k in [16, 32, 64, 128]:\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "    Z = None\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "\n",
    "    proj_test_dat = test_dat @ Z\n",
    "    acc = (clf.predict(proj_test_dat) == test_lab).mean()\n",
    "    print('Testing with embedding dimension',k)\n",
    "    print('Test accuracy of JL', 100*acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Random Projections and k-Means\n",
    "In k-means clustering, we partition a set of points $X$ into $k$ clusters $C_1,\\dots,C_k$ where in each cluster $C_i$ we choose the mean $\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$ as the cluster center.\n",
    "\n",
    "The cost of a clustering into clusters $C_1,\\dots,C_k$ is then $\\sum_{i=1}^k \\sum_{x \\in C_i}\\|x-\\mu_i\\|^2$.\n",
    "\n",
    "In the following, you may use without proof that the cost of cluster $C_i$ can be rewritten as $\\sum_{x \\in C_i}\\|x-\\mu_i\\|^2 = \\frac{1}{2|C_i|} \\sum_{x \\in C_i} \\sum_{y \\in C_i} \\|x - y\\|^2$.\n",
    "\n",
    "Argue that if we apply a projection to $X$ to obtain data set $X'$ in $d'$ dimensions, then if all pairwise distances between points in $X$ are preserved to within a multiplicative $(1 \\pm \\varepsilon)$ in $X'$, then the cost of *any* clustering (partitioning into $k$ clusters) is preserved to within a factor $(1 \\pm \\varepsilon)$ (that is, the cost of the cluster using points in $X'$ is within a factor $(1 \\pm \\varepsilon)$ of the cost when using $X$).\n",
    "\n",
    "The above implies that we may instead perform k-means clustering on $X'$ and still obtain high quality clusterings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4: Random Projections and Inner Products\n",
    "Recall that the many linear models we have seen in the course base their predictions on inner products $w^\\intercal x$. Moreover, recall from the lecture that the Johnson-Lindenstrauss transform guarantees (with high probability), for a set of feature vectors $x_1,\\dots,x_n \\in R^d$ that:\n",
    "$$\n",
    "\\|f(x_i) - f(x_j)\\|_2^2 \\in (1 \\pm \\varepsilon)\\|x_i - x_j\\|_2^2\n",
    "$$\n",
    "for all $x_i, x_j$. In this exercise, you must show that the Johnson-Lindenstrauss transform also preserves inner products with high probability. For this, we make an assumption that $-x_i$ is among $x_1,\\dots,x_n$ for every $x_i$ and that $f$ is linear.\n",
    "\n",
    "1. Assume $x_i$ and $x_j$ have unit length. Show that $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon$ when all distances are preserved to within $(1\\pm \\varepsilon)$. Hint: The identity $4 \\langle a, b\\rangle = \\|a+b\\|_2^2 - \\|a-b\\|_2^2$ may be useful. Also, for two unit vectors $a,b$ it holds that $\\|a+b\\|_2^2 \\leq 4$ and $\\|a-b\\|_2^2 \\leq 4$.\n",
    "\n",
    "2. Assume the embedding $f$ is linear (like the construction in the lecture), i.e. $f(a v) = af(v)$ for constants $a$ and let $x_i$ and $x_j$ be arbitrary. Assume all distances are preserved to within $(1\\pm \\varepsilon)$. Show that $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon \\|x_i\\|\\|x_j\\|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5: Random Projections and Support Vector Machines\n",
    "Recall from the support vector machines lectures that in the linearly separable case, we search for a hyperplane $w$ with the largest margin to the data. Assume there is such a hyperplane (let us ignore the bias $b$), specified by the normal vector $w$ of unit length, where the margin (geometric and functional since $\\|w\\| = 1$) to every training example is at least $\\gamma$. Assume furthermore that every training example $x_i$ has norm at most $R$.\n",
    "\n",
    "Show that a random projection into $k=C((R/\\gamma)^2 \\ln n)$ dimensions, for a big enough constant $C>0$, guarantees that the data is still linearly separable with high probability. Hint: It may be useful to use the properties proved in exercise 3 above and to use $\\varepsilon = \\gamma/(2R)$.\n",
    "\n",
    "We remark that since the VC-dimension of hyperplanes is $d+1$, the reduction to $k$ dimensions shows that a much smaller VC-dimension might suffice if there exists a separating hyperplane $w$ with large margins in $R^d$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
