{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: TFIDF and Co-Occurrences\n",
    "Consider the following training set of 5 strings:\n",
    "\n",
    "d1 = \"machine learning is my favourite new topic\"\n",
    "\n",
    "d2 = \"i really like support vector machines\"\n",
    "\n",
    "d3 = \"gradient descent is a really neat algorithm\"\n",
    "\n",
    "d4 = \"the lecturer has no imagination with these strings\"\n",
    "\n",
    "d5 = \"enjoy the very last TA class\"\n",
    "\n",
    "* In the tf-idf representation of string d1, what is the coordinate/entry corresponding to the word \"is\"?\n",
    "\n",
    "* If we consider a context window of width 1 around every word in every string, how many co-occurrences are there between \"the\" and \"is\"?\n",
    "\n",
    "### BEGIN MATH SOLUTION\n",
    "For the tf-idf question, we first compute the idf part. Here we see that \"is\" occcurs in 2 out of 5 document, hence the idf term is $\\ln(5/2) \\approx 0.92$. The tf term is $1/7$ as d1 has 7 words of which one is \"is\". In total, the tf-idf entry for \"is\" becomes $\\ln(5/2)/7 \\approx 0.13$.\n",
    "\n",
    "For the co-occurrences, we observe that \"the\" and \"is\" never co-occur within a context window of width 1. Hence it is 0.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: TFIDF Using standard tools \n",
    "In this exercise you must implement basic text classification pipeline and apply it to SMS spam classification.\n",
    "The pipeline is\n",
    "\n",
    "* Transform strings to vectors using a library implementation of tf-idf.\n",
    "* Apply a standard Logistic Regression classifier on the training data\n",
    "  You need to complete the implementation of **run_tfidf_classifier**\n",
    "\n",
    "\n",
    "See here for a default implementation of transforming text to TFIDF transformed vectors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "You may need to unzip the smsspamcollection file. For some reason, I need to run it twice after unzipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Probably not, still going over some stuff here\\n', 'I HAVE A DATE ON SUNDAY WITH WILL!!\\n', 'Thanks 4 your continued support Your question this week will enter u in2 our draw 4 Â£100 cash. Name the NEW US President? txt ans to 80082\\n', \"Dear 0776xxxxxxx U've been invited to XCHAT. This is our final attempt to contact u! Txt CHAT to 86688 150p/MsgrcvdHG/Suite342/2Lands/Row/W1J6HL LDN 18yrs\\n\", 'I sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\\n', 'Kothi print out marandratha.\\n', 'Arun can u transfr me d amt\\n', 'I asked you to call him now ok\\n', 'Ringtone Club: Gr8 new polys direct to your mobile every week !\\n', 'Hello! Just got here, st andrews-boy its a long way! Its cold. I will keep you posted\\n']\n",
      "(3734, 7097)\n",
      "train mean accuracy 0.9734868773433315, test mean accuracy 0.967391304347826\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_data():\n",
    "    #wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "    with open('smsspamcollection/SMSSpamCollection', 'r') as f:\n",
    "        dat = f.readlines()\n",
    "    labels = [x.split('\\t')[0] for x in dat]\n",
    "    texts = [x.split('\\t')[1] for x in dat]\n",
    "    sl = set(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def run_tfidf_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test using TFIDF transform\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    print(train_strings[0:10])\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    vectorizer.fit(train_strings)\n",
    "    X_train = vectorizer.transform(train_strings)\n",
    "    X_test = vectorizer.transform(test_strings)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    train_score =  clf.score(X_train, train_labels)\n",
    "    test_score = clf.score(X_test, test_labels)\n",
    "    print(f'train mean accuracy {train_score}, test mean accuracy {test_score}')\n",
    "    ### END CODE\n",
    "\n",
    "\n",
    "run_tfidf_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Analyzing Feature Hashing\n",
    "In feature hashing, we map a vector $x \\in R^d$ to a vector in $R^k$ using two hash functions $h : [d] \\to [k]$ and $g : [d] \\to \\{-1,1\\}$. The hash functions are chosen randomly and independently before seeing any data. Assume the hash functions satisfy the following two properties:\n",
    "1. For any two distinct coordinates $i \\neq j$, we have that $g(i)$ and $g(j)$ are independent and uniform random, i.e. for any $a,b \\in \\{-1,1\\}$ it holds that $\\Pr_g[g(i)=a \\wedge g(j)=b] = 1/4$.\n",
    "2. For any two distinct coordinates $i \\neq j$, we have that $\\Pr_h[h(i)=h(j)] \\leq 1/k$.\n",
    "\n",
    "The embedding $f(x)$ of a vector $x$ is obtained by hashing each index $i \\in [d]$ to the index $h(i)$ and adding $g(i) \\cdot x_i$ to $f(x)_{h(i)}$.\n",
    "\n",
    "Your task it to prove:\n",
    "1. For two vectors $x,y$, we have $\\mathbb{E}[f(x)^\\intercal f(y)] = x^\\intercal y$.\n",
    "\n",
    "Hint: The following re-writing may be useful:\n",
    "$$\n",
    "f(x)^\\intercal f(y) = \\sum_{i=1}^d \\sum_{j=1}^d 1_{[h(i)=h(j)]} x_i y_j g(i) g(j),\n",
    "$$\n",
    "where $1_{[h(i)=h(j)]}$ is the indicator random variable taking the value $1$ if $h(i)=h(j)$ and $0$ otherwise.\n",
    "You may also need linearity of expectation $\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]$ and that for independent random variables $X,Y$ we have $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.\n",
    "\n",
    "### BEGIN MATH SOLUTION\n",
    "1. Using the hint and linearity of expectation, we see that\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d \\sum_{j=1}^d \\mathbb{E}[1_{[h(i)=h(j)]} x_i y_j g(i) g(j)]\n",
    "$$\n",
    "Using that $h$ and $g$ are chosen independently and that $g(i)$ is independent of $g(j)$ when $i \\neq j$, it holds for any term with $i \\neq j$ that $\\mathbb{E}[1_{[h(i)=h(j)]} x_i y_j g(i) g(j)] = \\mathbb{E}[1_{[h(i)=h(j)]}] x_i y_j \\mathbb{E}[g(i)] \\mathbb{E}[g(j)]$. Since $g(i)$ is uniform random among $\\{-1,1\\}$, its expectation is $0$ and the whole term is $0$. Therefore we have\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d \\mathbb{E}[1_{[h(i)=h(i)]} x_i y_i g(i)^2]\n",
    "$$\n",
    "The indicator $1_{[h(i)=h(i)]}$ is always $1$ and $g(i)^2$ is also $1$. Hence\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d x_i y_i.\n",
    "$$\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4: Skip-Gram\n",
    "This exercise can be found in the separate notebook skipgram.ipynb. We recommend uploading the notebook to Google Colab for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
