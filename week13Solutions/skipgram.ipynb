{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exNZLXzdBWNg"
   },
   "source": [
    "## Ex4: Skip-Gram\n",
    "In this exercise, you must implement and train a Skip-Gram model. The data is provided in two .csv files. You can run the code on your own machine or you can choose to upload it to Google Colab. We suggest the latter as it supports GPU accelleration. Google Colab is about 4 times faster than my Mac Book Pro and training is still somewhat time consuming. Google Colab is free and it should be easy to get started. You can simply upload this notebook to Google Colab. Instructions for Google Colab follow here:\n",
    "\n",
    "Welcome to Google Colab \n",
    "\n",
    "To use the data use the tab on the left and click the bottom pane (folder icon) and use the upload to session storage button to upload data.csv and word_to_idx.csv.\n",
    "\n",
    "To enable gpu go the runtime fan and click Change runtime type and select GPU.\n",
    "\n",
    "Otherwise this works as a jupyter notebook. Run the code and see what happens. On our last run an epoch took around 143 seconds.\n",
    "\n",
    "Run 3 epochs or more.\n",
    "\n",
    "To implement:\n",
    "* You must implement the forward pass of the skip-gram neural net. Here we have included the application of softmax into the loss function. This means that your implementation should simply compute the output before applying softmax. Thus essentially you are implementing a neural net with one hidden layer, no bias and identity activation, both in the hidden and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spazgrECLVst"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "\n",
    "def get_dicts(path='word_to_idx.csv'):\n",
    "    df_dict = pd.read_csv(path)\n",
    "    print('See word index dataframe head')\n",
    "    print(df_dict.head(10))\n",
    "    idx_to_word = {idx: word for idx, word in zip(df_dict.idx, df_dict.word)}\n",
    "    word_to_idx = {word: idx for idx, word in zip(df_dict.idx, df_dict.word)}\n",
    "    return idx_to_word, word_to_idx\n",
    "\n",
    "def get_data(path='data.csv'):\n",
    "    df_dat = pd.read_csv(path)\n",
    "    print('See data dataframe head')\n",
    "    print(df_dat.head(10))\n",
    "    dat = torch.from_numpy(df_dat.X_in.values)\n",
    "    labels = torch.from_numpy(df_dat.X_out.values)\n",
    "    dataset = torch.utils.data.TensorDataset(dat, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, pin_memory=True)    \n",
    "    print('size of dataset:', len(dataset)) \n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "class KNN():\n",
    "    \"\"\" Simple K nearest neighbour data structure \"\"\"\n",
    "    def __init__(self, embedding, word_to_idx, idx_to_word):\n",
    "        print('Create KNN')\n",
    "        self.embedding = normalize(embedding.numpy(), axis=1)\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "\n",
    "    def query(self, idx, k=5):\n",
    "        tmp = {}\n",
    "        for i in idx:\n",
    "            tmp[i] = self.get_most_similar(i, k)\n",
    "        return tmp\n",
    "\n",
    "    def print_nearest(self, words, k=5):\n",
    "        for x in words:\n",
    "            idx = self.word_to_idx[x]\n",
    "            k_near_idx = self.get_most_similar(idx, k)\n",
    "            similar_words = [self.idx_to_word[z] for z in k_near_idx]\n",
    "            print('Most Similar to {0}:'.format(x), ', '.join(similar_words))\n",
    "\n",
    "    def get_most_similar(self, i, k):\n",
    "        \"\"\" Get the indexes of the most similar embedding vectors \n",
    "    \n",
    "            Args:\n",
    "                i: int\n",
    "                k: int\n",
    "            Returns \n",
    "                k_nearest: list    \n",
    "        \"\"\"\n",
    "        embed_i = self.embedding[i, :].reshape(-1, 1)\n",
    "        scores = (self.embedding @ embed_i).ravel()\n",
    "        ordered_sims = np.argsort(scores)[::-1]\n",
    "        k_nearest = ordered_sims[1:k + 1] # i is probably includes\n",
    "        assert ordered_sims[0] == i\n",
    "        return k_nearest\n",
    "\n",
    "def print_nearest(embedding, word_to_idx, idx_to_word):\n",
    "    knn = KNN(embedding, word_to_idx, idx_to_word)\n",
    "    test_words = [\"three\", \"cat\", \"city\", \"player\", \"king\", \"queen\"]\n",
    "    knn.print_nearest(test_words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf0Zcbg8S1QL",
    "outputId": "589d41c3-52c9-43fe-8d1e-e65427489845"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using:', device)\n",
    "\n",
    "class BasicSkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\" Trivial init\n",
    "        \"\"\"\n",
    "        super(BasicSkipGram, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = None # untrained\n",
    "        self.lossfunction = nn.CrossEntropyLoss()\n",
    "      \n",
    "    def forward(self, inputs, params):\n",
    "        \"\"\" Compute the forward pass\n",
    "            Note that we have put the application of softmax into the loss function\n",
    "            Thus the forward pass should only compute the output before taking softmax\n",
    "            Inputs consists of batch_size (denoted n) many indices i_1,...,i_n where i_j is the index of a word in the dictionary. \n",
    "            The corresponding one-hot encoding would be a vector with a 1 in position i_j and 0 elsewhere.\n",
    "            Output must have size n x num_embeddings\n",
    "        \"\"\"\n",
    "        embedding_mat = params['embedding'] # num_embeddings x embedding_dim\n",
    "        soft_layer = params['soft_layer'] # embedding_dim x num_embeddings\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        embeddings = embedding_mat[inputs, :] # n x embedding_dim\n",
    "        out = embeddings @ soft_layer\n",
    "        ### END CODE\n",
    "        return out\n",
    "\n",
    "    def loss(self, pred, labels):\n",
    "        return self.lossfunction(pred, labels)\n",
    "    \n",
    "    def train(self, train_loader,  epochs=1):\n",
    "        \"\"\" fit the neural net using CrossEntropyLoss\"\"\"\n",
    "        print('start training emb model')\n",
    "        ## Initialize parameters\n",
    "        train_embedding = torch.randn(net.num_embeddings, net.embedding_dim, device=device, requires_grad=True)\n",
    "        soft_layer = torch.randn(net.embedding_dim, net.num_embeddings, device=device, requires_grad=True)\n",
    "        params={'embedding': train_embedding, 'soft_layer': soft_layer}\n",
    "        print_steps = 10000\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "        ## Create GD optimizer + Adam is your friend\n",
    "        optimizer = optim.Adam(params.values())\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            batch_time = time.time()\n",
    "            for i, data in enumerate(train_loader):\n",
    "                _inputs, _labels = data\n",
    "                inputs = _inputs.to(device)\n",
    "                labels = _labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net.forward(inputs, params)\n",
    "                loss = self.loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if i % print_steps == print_steps - 1:\n",
    "                    batch_end_time = time.time()\n",
    "                    mean_loss = running_loss / print_steps\n",
    "                    print(f'Epoch {epoch}: batch {i+1} mean last {print_steps} loss: {mean_loss:.3f} - time used {time.time() - batch_time:.2f} s')\n",
    "                    running_loss = 0.0\n",
    "                    batch_time = time.time()\n",
    "            end_time = time.time()\n",
    "            print(f'\\nEpoch {epoch} finished - in {end_time - start_time:.2f} seconds')\n",
    "\n",
    "\n",
    "        print('\\nFinished Training')\n",
    "        self.embedding = train_embedding.detach()\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeZXIwCKYBrN",
    "outputId": "af8b71b8-6e40-48fd-bcd8-83d49bcd00a2"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "idx_to_word, word_to_idx = get_dicts(path='word_to_idx.csv')\n",
    "vocab_size = len(idx_to_word)\n",
    "print('vocabulary size', vocab_size)\n",
    "dataset, dataloader = get_data(path='data.csv')\n",
    "net = BasicSkipGram(num_embeddings=vocab_size, embedding_dim=embedding_dim)    \n",
    "net.train(dataloader, epochs=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlsi3s8kNW0T",
    "outputId": "312ef1da-b484-4035-e3d8-be18ff724e82"
   },
   "outputs": [],
   "source": [
    "print_nearest(net.embedding.cpu(), word_to_idx, idx_to_word)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
